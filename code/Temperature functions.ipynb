{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common functions for hanlding temperatures\n",
    "Includes:\n",
    "* Normal calculation functions\n",
    "* Degree-day calculation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import os\n",
    "# import glob\n",
    "\n",
    "import numpy as np\n",
    "import scipy.interpolate\n",
    "import pandas as pd\n",
    "\n",
    "# import matplotlib as mpl\n",
    "# import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "# import mpl_toolkits.axisartist\n",
    "# from mpl_toolkits.axes_grid1 import host_subplot\n",
    "\n",
    "# import dateutil\n",
    "\n",
    "# import statsmodels.formula.api as smformula\n",
    "\n",
    "# from collections import OrderedDict\n",
    "\n",
    "# from IPython.display import display, HTML\n",
    "# %matplotlib notebook\n",
    "# plt.style.use('seaborn-paper')\n",
    "\n",
    "# special-ish extensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Simple converters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tempF2C(x):\n",
    "    return (x-32.0)*5.0/9.0\n",
    "def tempC2F(x):\n",
    "    return (x*9.0/5.0)+32.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Functions for computing normals\n",
    "\n",
    "Example usage:\n",
    "given `tempdf` is a dataframe or series of hourly temperature values:  \n",
    "\n",
    "```python\n",
    "tempnorm, tempresamp = compute_year_over_year_norm(tempdf,\n",
    "                                                   START_DATE, END_DATE,\n",
    "                                                   norm_end=END_DATE,\n",
    "                                                   freq='hourly',\n",
    "                                                   interp_method='linear')\n",
    "                                                   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_year_over_year_norm(in_dataframe,\n",
    "                                start, end,\n",
    "                                norm_start=None, norm_end=None,\n",
    "                                freq='daily',\n",
    "                                interp_method='linear',\n",
    "                                quiet=False):\n",
    "    \"\"\"\n",
    "    Will propagate the timezone of in_dataframe.index to the return values\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    start: convertable to Datetime\n",
    "        start range of dates to output\n",
    "    end: convertable to Datetime\n",
    "        end range of dates to output\n",
    "    norm_start : convertable to Datetime or None\n",
    "        `None` will use in_dataframe.index[0]\n",
    "    norm_end : convertable to Datetime or None\n",
    "        if given (not None), output range does not include `norm_end` (it is half-open)\n",
    "        `None` will use in_dataframe.index[-1]\n",
    "    freq : {'daily', 'hourly'}\n",
    "    interp_method : str or None\n",
    "        `None` will skip resample and interpolation, so \n",
    "        `in_dataframe` must already be daily or hourly (depending on `freq`)!\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    norm, resampled_in_dataframe\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    tempnorm, tempresamp = compute_year_over_year_norm(tempdf,\n",
    "                                                   START_DATE, END_DATE,\n",
    "                                                   norm_end=END_DATE,\n",
    "                                                   freq='hourly',\n",
    "                                                   interp_method='linear',\n",
    "                                                   quiet=False)\n",
    "    \"\"\"\n",
    "    if freq == 'hourly':\n",
    "        hrs = 24\n",
    "        hrs_freq = '1h'\n",
    "    elif freq == 'daily':\n",
    "        hrs = 1\n",
    "        hrs_freq = '24h'\n",
    "    else:\n",
    "        raise ValueError(\"Invalid `freq` argument value: {}\".format(freq))\n",
    "    if norm_start is None:\n",
    "        norm_start = in_dataframe.index[0]\n",
    "    else:\n",
    "        norm_start = pd.to_datetime([norm_start])[0].tz_localize(in_dataframe.index.tz)# - pd.Timedelta('1 second')\n",
    "    if norm_end is None:\n",
    "        norm_end = in_dataframe.index[-1]\n",
    "    else:\n",
    "        norm_end = pd.to_datetime([norm_end])[0].tz_localize(in_dataframe.index.tz)# - pd.Timedelta('1 second')\n",
    "    \n",
    "    if not quiet:\n",
    "        print('Computing using range:', norm_start, 'to', norm_end)\n",
    "    \n",
    "    if interp_method is None: # skip resample+interpolation (assumes in_dataframe is daily!)\n",
    "        t = in_dataframe.loc[norm_start:norm_end]\n",
    "    else: # resample and interpolate to get hourly\n",
    "        # pandas resample: simple but might require upsampling in some cases\n",
    "        # t = in_dataframe.resample(hrs_freq).interpolate(method=interp_method).loc[norm_start:norm_end]\n",
    "        # using scipy.interpolate.interp1d: seems safer / more reliable than pandas resample\n",
    "        nidx = pd.date_range(norm_start, norm_end, freq=hrs_freq)\n",
    "        if isinstance(in_dataframe, pd.DataFrame):\n",
    "            t = pd.DataFrame([], index=nidx)\n",
    "            for col in in_dataframe:   \n",
    "                t[col] = scipy.interpolate.interp1d(in_dataframe.index.astype('int64').values, \n",
    "                                                    in_dataframe[col].values, \n",
    "                                                    kind=interp_method, \n",
    "                                                    fill_value=np.nan, #(0,1) \n",
    "                                                    bounds_error=False)(t.index.astype('int64').values)\n",
    "        elif isinstance(in_dataframe, pd.Series):\n",
    "            t = pd.Series(scipy.interpolate.interp1d(\n",
    "                                        in_dataframe.index.astype('int64').values, \n",
    "                                        in_dataframe.values, \n",
    "                                        kind=interp_method,\n",
    "                                        fill_value=np.nan, #(0,1) \n",
    "                                        bounds_error=False)(nidx.astype('int64').values),\n",
    "                          index=nidx)\n",
    "\n",
    "        else:\n",
    "            assert False, \"in_dataframe must be a pandas.DataFrame or pandas.Series\"\n",
    "            \n",
    "\n",
    "    # actual norm calculation\n",
    "    norm = t.groupby([t.index.month, t.index.day, t.index.hour]).mean().sort_index()\n",
    "\n",
    "    # now replicate and trim to the desired output range\n",
    "    start = pd.to_datetime(start).tz_localize(in_dataframe.index.tz)\n",
    "    end = pd.to_datetime(end).tz_localize(in_dataframe.index.tz)\n",
    "    \n",
    "    # need a non-leapyear and leapyear version\n",
    "    norm_ly = norm.copy()\n",
    "    if norm.shape[0] == 366*hrs:\n",
    "        norm = norm.drop((2,29,))\n",
    "    else: # norm doesn't include any leapyear data\n",
    "        assert norm.shape[0] == 365*hrs\n",
    "        # make Feb 29 the mean of Feb 28 and Mar 1\n",
    "        foo = (norm.loc[(2,28,)] + norm.loc[(3,1,)]) / 2.0\n",
    "        foo.index = pd.MultiIndex.from_product( ([2],[29],list(range(hrs))) )\n",
    "        norm_ly = pd.concat((norm_ly,foo)).sort_index()\n",
    "        norm_ly.sort_index(inplace=True) # probably not needed\n",
    "\n",
    "    # build up a 'long normal' (lnorm) dataframe year by year by appending the norm or norm_ly\n",
    "    lnorm = None\n",
    "    for yr in np.arange(start.year, end.year+1):\n",
    "        idx = pd.date_range(start='{}-{:02d}-{:02d} {:02d}:00:00'.format(yr,*norm.index[0]),\n",
    "                            end=  '{}-{:02d}-{:02d} {:02d}:00:00'.format(yr,*norm.index[-1]),\n",
    "                            freq=hrs_freq,\n",
    "                            tz=in_dataframe.index.tz)\n",
    "        if idx.shape[0] == 366*hrs:\n",
    "            foo = norm_ly.copy()\n",
    "        else:\n",
    "            assert norm.shape[0] == 365*hrs\n",
    "            foo = norm.copy()\n",
    "        foo.index = idx\n",
    "        if lnorm is None:\n",
    "            lnorm = foo\n",
    "        else:\n",
    "            lnorm = lnorm.append(foo)\n",
    "\n",
    "    return lnorm.loc[start:end], t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claculate degree days and threshold crossings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for computing Degree-day\n",
    "#### Compute the number of degree days (DD) for each day in the temperature file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Used internally\n",
    "def compute_threshold_passings(ddvals, cumulative_DD_threshold):\n",
    "    \"\"\"Find the index where cumsum of ddvals starting at each index passes the cumulative_DD_threshold\n",
    "    ddvals needs to be a pandas Series or DataFrame column\"\"\"\n",
    "    crossing_idx_rv = np.zeros(len(ddvals))*np.nan\n",
    "    cDD = ddvals.cumsum(skipna=True)\n",
    "    crossing_idx_rv = np.searchsorted(cDD, cDD+cumulative_DD_threshold-ddvals, side='left').astype(float)\n",
    "    crossing_idx_rv[crossing_idx_rv==len(crossing_idx_rv)] = np.nan \n",
    "    #crossing_idx_rv[np.array(pd.isnull(ddvals),dtype=bool)] = np.nan  # @TCC Does not propagate nan correctly\n",
    "    return crossing_idx_rv #, cdd_rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _ddvals_to_outdf(dd, dd_threshold):\n",
    "    \"\"\" used internally\n",
    "    prepare nice output dataframe from ddvals\n",
    "    \"\"\"\n",
    "    # Find the point where cumulative sums of degree days cross the threshold\n",
    "    dd['idx'] = compute_threshold_passings(dd['DD'], dd_threshold)\n",
    "    # convert those indexes into end times\n",
    "    i = dd['idx'] # just convinience\n",
    "    e = pd.Series(index=dd.index, dtype='datetime64[ns]')\n",
    "    e[i.notnull()] = dd.index[i[i.notnull()].astype(int)]\n",
    "    dd['end'] = e\n",
    "    # and duration... \n",
    "    dd['dur'] = dd['end']-dd.index+pd.Timedelta(days=1)\n",
    "    dd['dur_days'] = dd['dur'].apply(lambda x: np.nan if pd.isnull(x) else x.days)\n",
    "    return dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_summation_DD_from_hourly_df(temperature_df, base_temp, dd_threshold):\n",
    "    \"\"\"hourly summation degree-day method\n",
    "    to compute when a cumulative threshold has been passed \n",
    "    given hourly temperature data.\n",
    "    \n",
    "    NOTE: Will actually work with non-hourly and non-equally-spaced data, \n",
    "    but accuracy will be lower for lower frequency and does not handle readings which cross midnight properly.\n",
    "    \"\"\"\n",
    "    # so we don't accidently mess up the input dataframe/series\n",
    "    t = pd.DataFrame(data=temperature_df.copy(deep=True), index=temperature_df.index)\n",
    "    # difference in datatime in units of (float) hours\n",
    "    t['dDays'] = t.index\n",
    "    t['dDays'] = t['dDays'].diff().dt.total_seconds() / (24*60*60)\n",
    "    # temp above threshold (or 0 if below)\n",
    "    t['dAT'] = t['AT']-DD_BASE_TEMP\n",
    "    t.loc[t['dAT'] < 0, 'dAT'] = 0\n",
    "    # thermal accumulation for each time period\n",
    "    t['DD'] = t['dAT']*t['dDays']\n",
    "    # compute the degree-days for each day\n",
    "    dd = t['DD'].groupby(pd.TimeGrouper('D')).sum().to_frame(name='DD')\n",
    "    return _ddvals_to_outdf(dd, dd_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_single_triangle_DD_from_hourly_df(temperature_df, base_temp, dd_threshold):\n",
    "    \"\"\"single triangle degree-day method\n",
    "    to compute when a cumulative threshold has been passed \n",
    "    given hourly temperature data.\n",
    "    \"\"\"\n",
    "    # so we don't accidently mess up the input dataframe/series\n",
    "    t = pd.DataFrame(data=temperature_df.copy(deep=True), index=temperature_df.index)\n",
    "\n",
    "    # compute the degree-days for each day in the temperature input\n",
    "    dfgrp = t.groupby(pd.TimeGrouper('D'))\n",
    "    dd = dfgrp.agg(lambda x: _compute_daily_SingleTri_DD_from_hourly_groupby(x, base_temp))\n",
    "    dd.columns = ['DD']\n",
    "    return _ddvals_to_outdf(dd, dd_threshold)\n",
    "\n",
    "# Used internally\n",
    "def _compute_daily_SingleTri_DD(mint, maxt, avet, base_temp):\n",
    "    \"\"\"Use standard Baskerville-Ermin (single sine) degree-day method\n",
    "    to compute the degree-day values for each a single day.\n",
    "    \"\"\"\n",
    "    dd = np.nan # value which we're computing\n",
    "    # Adjust for observation time; not relevant\n",
    "    # if max < base (curve all below base)\n",
    "    if maxt <= base_temp:\n",
    "        dd = 0\n",
    "    # min <= base; tri from max to base_temp\n",
    "    elif mint <= base_temp:\n",
    "        dd = (maxt-base_temp)*(maxt-base_temp)*(0.5/(maxt-mint))\n",
    "    # else whole tri extends above; tri from max to min + rect from min to base\n",
    "    else:\n",
    "        assert mint > base_temp\n",
    "        dd = (maxt-mint)/2.0 + (mint-base_temp)\n",
    "    return dd\n",
    "\n",
    "def _compute_daily_SingleTri_DD_from_hourly_groupby(grp, base_temp):\n",
    "    if grp.isnull().values.any():\n",
    "        return np.nan\n",
    "    mint = np.min(grp)\n",
    "    maxt = np.max(grp)\n",
    "    avet = (mint+maxt)/2.0 # simple midpoint (like in the refs)\n",
    "    assert not np.isnan(mint)\n",
    "    return _compute_daily_SingleTri_DD(\n",
    "                            mint=mint,\n",
    "                            maxt=maxt,\n",
    "                            avet=avet,\n",
    "                            base_temp=base_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_BM_DD_from_hourly_df(temperature_df, base_temp, dd_threshold):\n",
    "    \"\"\"Use standard Baskerville-Ermin (single sine) degree-day method\n",
    "    to compute when a cumulative threshold has been passed \n",
    "    given hourly temperature data.\n",
    "    (though its value is computed using daily min & max temperatures)\n",
    "    \"\"\"\n",
    "    # so we don't accidently mess up the input dataframe/series\n",
    "    t = pd.DataFrame(data=temperature_df.copy(deep=True), index=temperature_df.index)\n",
    "\n",
    "    # compute the degree-days for each day in the temperature input\n",
    "    dfgrp = t.groupby(pd.TimeGrouper('D'))\n",
    "    dd = dfgrp.agg(lambda x: _compute_daily_BM_DD_from_hourly_groupby(x, base_temp))\n",
    "    dd.columns = ['DD']\n",
    "    return _ddvals_to_outdf(dd, dd_threshold)\n",
    "\n",
    "def compute_BM_DD_from_daily_min_and_max(mint, maxt, base_temp, dd_threshold):\n",
    "    dd = pd.DataFrame({'minT':mint, 'maxT':maxt}, index=mint.index)\n",
    "    dd['DD'] = pd.concat([mint,maxt], axis=1).apply(lambda x: \n",
    "                    _compute_daily_BM_DD(x[0], x[1], (x[0]+x[1])/2.0, DD_BASE_TEMP),\n",
    "                    axis=1)\n",
    "    return _ddvals_to_outdf(dd, dd_threshold)\n",
    "\n",
    "# Used internally\n",
    "def _compute_daily_BM_DD(mint, maxt, avet, base_temp):\n",
    "    \"\"\"Use standard Baskerville-Ermin (single sine) degree-day method\n",
    "    to compute the degree-day values for each a single day.\n",
    "    \"\"\"\n",
    "    dd = np.nan # value which we're computing\n",
    "    # Step 1: Adjust for observation time; not relevant\n",
    "    # Step 2: GDD = 0 if max < base (curve all below base)\n",
    "    if maxt < base_temp:\n",
    "        dd = 0\n",
    "    # Step 3: Calc mean temp for day; already done previously\n",
    "    # Step 4: min > base; then whole curve counts\n",
    "    elif mint >= base_temp:\n",
    "        dd = avet - base_temp\n",
    "    # Step 5: else use curve minus part below base\n",
    "    else:\n",
    "        W = (maxt-mint)/2.0\n",
    "        tmp = (base_temp-avet) / W\n",
    "        if tmp < -1:\n",
    "            print('WARNING: (base_temp-avet)/W = {} : should be [-1:1]'.format(tmp))\n",
    "            tmp = -1\n",
    "        if tmp > 1:\n",
    "            print('WARNING: (base_temp-avet)/W = {} : should be [-1:1]'.format(tmp))\n",
    "            tmp = 1\n",
    "        A = np.arcsin(tmp)\n",
    "        dd = ((W*np.cos(A))-((base_temp-avet)*((np.pi/2.0)-A)))/np.pi\n",
    "    return dd\n",
    "\n",
    "def _compute_daily_BM_DD_from_hourly_groupby(grp, base_temp):\n",
    "    mint = np.min(grp)\n",
    "    maxt = np.max(grp)\n",
    "    avet = (mint+maxt)/2.0 # simple midpoint (like in the refs)\n",
    "    return _compute_daily_BM_DD(\n",
    "                            mint=mint,\n",
    "                            maxt=maxt,\n",
    "                            avet=avet,\n",
    "                            base_temp=base_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Cython version of compute_threshold_passings\n",
    "## might be slightly faster (numpy.searchsorted is pretty good though)\n",
    "## However, makes portablility more complicated\n",
    "# %load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%cython\n",
    "# # Used internally\n",
    "# import numpy as np\n",
    "# cimport numpy as np\n",
    "# import cython\n",
    "# from libc.math cimport isnan\n",
    "\n",
    "# def compute_threshold_passings(double[:] ddvals, double cumulative_DD_threshold):\n",
    "#     cdef double cDD\n",
    "#     cdef int found\n",
    "#     cdef double[:] crossing_idx_rv = np.zeros(len(ddvals))*np.nan\n",
    "#     #cdef double[:] cdd_rv = np.zeros(len(ddvals))*np.nan\n",
    "\n",
    "#     # @TCC -- Probably don't need to do this for all possible start dates... \n",
    "#     for i in range(len(ddvals)):\n",
    "#         cDD = 0\n",
    "#         found = 0\n",
    "#         for j in range(i,len(ddvals)):\n",
    "#             #print(start_d.Index, d.Index, cDD)\n",
    "#             if isnan(ddvals[i]):\n",
    "#                 crossing_idx_rv[i] = np.nan\n",
    "#                 found = 1 # we might find more later\n",
    "#                 break\n",
    "#             else:\n",
    "#                 cDD += ddvals[j]\n",
    "#                 if cDD > cumulative_DD_threshold:\n",
    "#                     found = 1\n",
    "#                     crossing_idx_rv[i] = j\n",
    "#                     #dd_rv[i] = cDD # @TCC Not really needed\n",
    "#                     break\n",
    "#         if found != 1: # won't find any more either\n",
    "#             break\n",
    "\n",
    "#     return crossing_idx_rv #, cdd_rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
