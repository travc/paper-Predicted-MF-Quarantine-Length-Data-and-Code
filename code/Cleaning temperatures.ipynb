{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning resampling ISH temperature datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# boilerplate includes\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import datetime\n",
    "import scipy.interpolate\n",
    "# import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "%matplotlib notebook\n",
    "plt.style.use('seaborn-notebook')\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants / Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PARAMETERS (might be overridden by a calling script)\n",
    "\n",
    "# if not calling from another script (batch), SUBNOTEBOOK_FLAG might not be defined\n",
    "try:\n",
    "    SUBNOTEBOOK_FLAG\n",
    "except NameError:\n",
    "    SUBNOTEBOOK_FLAG = False\n",
    "    \n",
    "# Not calling as a sub-script? define params here\n",
    "if not SUBNOTEBOOK_FLAG:\n",
    "    \n",
    "    # SET PARAMETER VARIABLES HERE UNLESS CALLING USING %run FROM ANOTHER NOTEBOOK\n",
    "    \n",
    "    DATADIR = '../data/temperatures/ISD'\n",
    "    OUTDIR = '../data/temperatures'\n",
    "\n",
    "    FTPHOST = 'ftp.ncdc.noaa.gov'\n",
    "    FETCH_STATIONS_LIST_FILE = True\n",
    "\n",
    "    TEMP_COL = 'AT' # The label of the hourly temperature column we make/output\n",
    "\n",
    "    # Resampling and interpolation parameters\n",
    "    # spline order used for converting to on-the-hour and filling small gaps\n",
    "    BASE_INTERPOLATION_K = 1 # 1 for linear interpolation\n",
    "    # give special treatment to data gaps longer than...\n",
    "    POTENTIALLY_PROBLEMATIC_GAP_SIZE = pd.Timedelta('03:00:00')\n",
    "\n",
    "    # Time range to use for computing normals (30 year, just like NOAA uses)\n",
    "    NORM_IN_START_DATE = '1986-07-01'\n",
    "    NORM_IN_END_DATE = '2016-07-01'\n",
    "    # Time range or normals to output to use when running 'medfoes on normal temperature' (2 years, avoiding leapyears)\n",
    "    NORM_OUT_START_DATE = '2014-01-01'\n",
    "    NORM_OUT_END_DATE = '2015-12-31 23:59:59'\n",
    "    \n",
    "print(\"Cleaning temperature data for \",STATION_CALLSIGN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Potentially turn interactive figure display off\n",
    "if SUPPRESS_FIGURE_DISPLAY:\n",
    "    plt.ioff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolation and cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "fn = \"{}_AT.h5\".format(STATION_CALLSIGN)\n",
    "ot = pd.read_hdf(os.path.join(DATADIR,fn), 'table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deduplication\n",
    "More precisely, we can only have one value for each time,\n",
    "otherwise interpolation doesn't make much sense (or work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = ot.copy(deep=True) # not needed, just safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# just showing the duplicates\n",
    "tmp = t[t.index.duplicated(keep=False)].sort_index()\n",
    "print(len(tmp), 'duplicates')\n",
    "#display(tmp) # decomment to see the list of duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# actually remove duplicates, just keeping the first\n",
    "# @TCC could somehow try to identify the most reliable or take mean or such\n",
    "t = t[~t.index.duplicated(keep='first')].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier removal\n",
    "Using a deviation from running median/sigam threshold method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fairly permissive settings\n",
    "rolling_sigma_window = 24*5 # None or 0 to just use median instead of median/sigma\n",
    "rolling_median_window = 5\n",
    "thresh = 1.5 # deviation from media/sigma to trigger removal\n",
    "multipass = True # cycle until no points removed, or False for not\n",
    "\n",
    "tin = t\n",
    "cum_num = 0\n",
    "while multipass:\n",
    "    if rolling_sigma_window:\n",
    "        sigma = t['AT'].rolling(window=rolling_sigma_window, center=True).std()\n",
    "    else:\n",
    "        sigma = 1\n",
    "    diff = (t['AT']-t['AT'].rolling(window=rolling_median_window, center=True).median())/sigma\n",
    "\n",
    "    outlier_mask = diff.abs() > thresh\n",
    "    num = np.count_nonzero(outlier_mask)\n",
    "    cum_num += num\n",
    "    print(\"removing {} points\".format(num))\n",
    "    if num == 0:\n",
    "        break\n",
    "\n",
    "    # plotting each step\n",
    "#     ax = t.plot(linestyle='-', marker='*')\n",
    "#     if np.count_nonzero(outlier_mask) > 0:\n",
    "#         t[outlier_mask].plot(ax=ax, linestyle='none', marker='o', color='red')\n",
    "#     diff.abs().plot(ax=ax)\n",
    "#     if np.count_nonzero(outlier_mask) > 0:\n",
    "#         diff.abs()[outlier_mask].plot(ax=ax, linestyle='none', marker='o', color='yellow')\n",
    "\n",
    "    t = t[~outlier_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot showing what is being removed\n",
    "if cum_num > 0:\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax = tin[~tin.index.isin(t.index)].plot(ax=ax, linestyle='none', marker='o', color='r', zorder=8)\n",
    "    ax = tin.plot(ax=ax, linestyle='-', linewidth=1, marker=None, color='red')\n",
    "    ax = t.plot(ax=ax, linestyle='-', marker='.', color='blue')\n",
    "    ax.set_ylabel('air temperature [$\\degree$ C]')\n",
    "    ax.legend(['outlier', 'original', 'cleaned'])\n",
    "    ax.set_title(STATION_CALLSIGN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# saving figure\n",
    "# saving\n",
    "fn = '{}_outlier.png'.format(STATION_CALLSIGN)\n",
    "fig.savefig(os.path.join(OUTDIR,fn))\n",
    "#mpld3.save_html(fig, '{}_outler.html'.format(STATION_CALLSIGN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Actually apply the outlier removal\n",
    "ot = t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"by-hand\" fixes for particular datasets, hopefully minimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_spurious_temps(ot, query_op, date1, date2=None, plot=True, inplace=False):\n",
    "    if date2 is None:\n",
    "        date2 = date1\n",
    "    ax = ot.loc[date1:date2].plot(ax=None, linestyle='-', marker='o') # plot\n",
    "    out_t = ot.drop(ot.loc[date1:date2].query('AT {}'.format(query_op)).index, inplace=inplace)\n",
    "    if inplace:\n",
    "        out_t = ot\n",
    "    out_t.loc[date1:date2].plot(ax=ax, linestyle='-', marker='*') # plot'\n",
    "    ax.set_title(\"Remove AT {}, range=[{}:{}]\".format(query_op, date1, date2))\n",
    "    return out_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "STATION_CALLSIGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if STATION_CALLSIGN == 'KSNA': # KSNA (Orange County)\n",
    "    # 2016-08-14 to 2016-08-15 overnight has some >0 values when they should be more like 19-20\n",
    "    remove_spurious_temps(ot, '< 0', '2016-08-14', '2016-08-15', inplace=True)\n",
    "    \n",
    "if STATION_CALLSIGN == 'KSFO':\n",
    "    remove_spurious_temps(ot, '< 0', '1976-07-16', '1976-07-17', inplace=True)\n",
    "\n",
    "if STATION_CALLSIGN == 'KRIV':\n",
    "    remove_spurious_temps(ot, '< 0', '1995-11-15', '1995-11-15', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify bigger gaps which will get filled day-over-day interpolation\n",
    "Interpolate based on same hour-of-day across days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# flag the gaps in the original data that are possibly too long for the simple interpolation we did above\n",
    "gaps_filename = os.path.join(OUTDIR, \"{}_AT_gaps.tsv\".format(STATION_CALLSIGN))\n",
    "gaps = ot.index.to_series().diff()[1:]\n",
    "idx = np.flatnonzero(gaps > POTENTIALLY_PROBLEMATIC_GAP_SIZE)\n",
    "prob_gaps = gaps[idx]\n",
    "# save to file for future reference\n",
    "with open(gaps_filename,'w') as fh:\n",
    "    # output the gaps, biggest to smallest, to review\n",
    "    print('#', STATION_CALLSIGN, ot.index[0].isoformat(), ot.index[-1].isoformat(), sep='\\t', file=fh)\n",
    "    print('# Potentially problematic gaps:', len(prob_gaps), file=fh)\n",
    "    tmp = prob_gaps.sort_values(ascending=False)\n",
    "    for i in range(len(tmp)):\n",
    "        rng = [tmp.index[i]-tmp.iloc[i], tmp.index[i]]\n",
    "        print(rng[0], rng[1], rng[1]-rng[0], sep='\\t', file=fh)\n",
    "\n",
    "if not SUPPRESS_FIGURE_DISPLAY:\n",
    "    # go ahead and just print it here too\n",
    "    with open(gaps_filename) as fh:\n",
    "        for l in fh:\n",
    "            print(l, end='')\n",
    "else:\n",
    "    print('# Potentially problematic gaps:', len(prob_gaps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate to produce on-the-hour values\n",
    "Simple interpolation hour-to-hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Interpolate to get on-the-hour values\n",
    "newidx = pd.date_range(start=ot.index[0].round('d')+pd.Timedelta('0h'),\n",
    "                       end=ot.index[-1].round('d')-pd.Timedelta('1s'),\n",
    "                       freq='1h', tz='UTC')\n",
    "\n",
    "if True:\n",
    "    # Simple linear interpolation\n",
    "    at_interp_func = scipy.interpolate.interp1d(ot.index.astype('int64').values, \n",
    "                                           ot['AT'].values, \n",
    "                                           kind='linear', \n",
    "                                           fill_value=np.nan, #(0,1) \n",
    "                                           bounds_error=False)\n",
    "else:\n",
    "    # Should be better method, but has some screwy thing using updated data\n",
    "    at_interp_func = scipy.interpolate.InterpolatedUnivariateSpline(\n",
    "        ot.index.astype('int64').values, \n",
    "        ot['AT'].values, \n",
    "        k=BASE_INTERPOLATION_K,\n",
    "        ext='const')\n",
    "\n",
    "nt = pd.DataFrame({'AT':at_interp_func(newidx.astype('int64').values)},\n",
    "                   index=newidx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill the bigger gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fill those gaps using day-to-day (at same hour) interpolation\n",
    "gap_pad = pd.Timedelta('-10m') # contract the gaps a bit so we don't remove good/decent edge values\n",
    "\n",
    "t = nt.copy(deep=True) # operate on a copy so we can compare with nt\n",
    "# fill the gap ranges with nan (replacing the default interpolation)\n",
    "for i in range(len(prob_gaps)):\n",
    "    rng = [prob_gaps.index[i]-prob_gaps.iloc[i], prob_gaps.index[i]]\n",
    "    t[rng[0]-gap_pad:rng[1]+gap_pad] = np.nan\n",
    "\n",
    "# reshape so each row is a whole day's (24) data points\n",
    "rows = int(t.shape[0]/24)\n",
    "foo = pd.DataFrame(t.iloc[:rows*24].values.reshape((rows,24)))\n",
    "\n",
    "# simple linear interpolation\n",
    "foo.interpolate(metnod='time', limit=24*60, limit_direction='both', inplace=True)\n",
    "\n",
    "# # Alternative interpolation using running means\n",
    "# # @TCC not great for very large gaps\n",
    "# RUNNING_MEAN_WINDOW_SIZE = 3\n",
    "# while True:\n",
    "#     # interpolate each column (temp at hour x on each day)\n",
    "#     # filling nans with values from a windowed running mean\n",
    "#     foo.fillna(foo.rolling(window=RUNNING_MEAN_WINDOW_SIZE, min_periods=1, center=True).mean(), inplace=True)\n",
    "#     if not foo.isnull().values.any():\n",
    "#         break\n",
    "\n",
    "# reshape back\n",
    "t = pd.DataFrame({'AT':foo.stack(dropna=False).values}, index=t.index[:rows*24])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check that it looks OK...\n",
    "### Plot the temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You can specify a specific range by setting r1 and r2, or None for full range\n",
    "#r1, r2 = '1952-05-07', '1952-05-23'\n",
    "r1, r2 = None, None\n",
    "\n",
    "if r1 is None:\n",
    "    r1 = t.index[0]\n",
    "if r2 is None:\n",
    "    r2 = t.index[-1]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(ot.loc[r1:r2].index, ot.loc[r1:r2]['AT'], linestyle='none', marker='.', label='raw')\n",
    "#ax.scatter(ot.loc[r1:r2].index, ot.loc[r1:r2]['AT'], marker='.', label='raw')\n",
    "ax.plot(nt.loc[r1:r2].index, nt.loc[r1:r2]['AT'], linestyle='-', marker=None, lw=1, label='interpolated')\n",
    "\n",
    "# ax.plot(t.loc[r1:r2].index, t.loc[r1:r2]['AT'], '-*', lw=1, label='filled')\n",
    "\n",
    "# @TCC maybe make a single dataframe with the parts I don't want deleted or masked out\n",
    "for i in range(len(prob_gaps)):\n",
    "    if i == 0: # only label first segment\n",
    "        label = 'filled'\n",
    "    else:\n",
    "        label = ''\n",
    "    rng = [tmp.index[i]-tmp.iloc[i], tmp.index[i]]\n",
    "    ax.plot(t.loc[rng[0]:rng[1]].index, t.loc[rng[0]:rng[1]]['AT'], '.-', lw=1, color='r', label=label)\n",
    "\n",
    "# # mark the big gaps with vertical lines\n",
    "# for i in range(len(prob_gaps)):\n",
    "#     ax.axvline(prob_gaps.index[i]-prob_gaps.iloc[i],\n",
    "#                c='k', ls=':', lw=0.5)\n",
    "#     ax.axvline(prob_gaps.index[i],\n",
    "#                c='k', ls=':', lw=0.5)\n",
    "    \n",
    "ax.set_xlim((r1,r2))\n",
    "ax.set_xlabel('DateTime')\n",
    "ax.set_ylabel('Temperature [$\\degree$C]')\n",
    "ax.set_title(STATION_CALLSIGN)\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# saving\n",
    "fig.savefig(os.path.join(OUTDIR, '{}_cleaning.png'.format(STATION_CALLSIGN)))\n",
    "#mpld3.save_html(fig, '{}_cleaning.html'.format(STATION_CALLSIGN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save final cleaned temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outfn = os.path.join(OUTDIR, \"{}_AT_cleaned\".format(STATION_CALLSIGN))\n",
    "print(\"Saving cleaned temp data to:\", outfn)\n",
    "t.to_hdf(outfn+'.h5', 'table', mode='w',\n",
    "          data_colums=True, complevel=5, complib='bzip2',\n",
    "          dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the normals\n",
    "Need the normal (repated so it covers 2 years) for running medfoes on the normals\n",
    "\n",
    "Not needed for this particular study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Time range to use for computing normals (30 year, just like NOAA uses)\n",
    "# NORM_IN_START_DATE = '1986-07-01'\n",
    "# NORM_IN_END_DATE = '2016-07-01'\n",
    "# # Time range or normals to output to use when running 'medfoes on normal temperature' (2 years, avoiding leapyears)\n",
    "# NORM_OUT_START_DATE = '2014-01-01'\n",
    "# NORM_OUT_END_DATE = '2015-12-31 23:59:59'\n",
    "\n",
    "# %run \"Temperature functions.ipynb\" # for compute_year_over_year_norm function\n",
    "\n",
    "# tempnorm = compute_year_over_year_norm(ot,\n",
    "#                                        NORM_OUT_START_DATE, NORM_OUT_END_DATE,\n",
    "#                                        NORM_IN_START_DATE, NORM_IN_END_DATE,\n",
    "#                                        freq='hourly',\n",
    "#                                        interp_method='linear',\n",
    "#                                        norm_method='mean')\n",
    "\n",
    "# # Save as csv for medfoes input\n",
    "# outfn = os.path.join(OUTDIR, \"{}_AT_cleaned_normalsX2.csv\".format(STATION_CALLSIGN))\n",
    "# print(\"Saving temp normals data to:\",outfn)\n",
    "# tempnorm.to_csv(outfn, index_label='datetime')\n",
    "\n",
    "# tempnorm.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn iteractive display back on, if we turned it off\n",
    "if SUPPRESS_FIGURE_DISPLAY:\n",
    "    plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
